{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "main_finetuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicordaro/Project_MLDL/blob/master/main_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqazEUOdZpEb",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "fNgxhFrmdm7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Github folder\n",
        "import os\n",
        "# if os.path.isdir('./Project_MLDL'):\n",
        "!rm -rf Project_MLDL\n",
        "if not os.path.isdir('./CIFAR100_tError'):\n",
        "  !git clone https://github.com/Nicordaro/Project_MLDL\n",
        "  \n",
        "import copy\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image\n",
        "from Project_MLDL.confusion_matrix import *\n",
        "from Project_MLDL.CIFAR100_tError import CIFAR100_tError\n",
        "from Project_MLDL.model_finetuning import ResNet18\n",
        "from scipy import interpolate\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet34\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piUl6RHQ7-nu",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p_MAWlv7-00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_data_labels(input_list):\n",
        "  \"\"\"\n",
        "  Given an input list as argument, this function returns two lists:\n",
        "  the first containing all elements of the input list except for the first 10\n",
        "  the second containing the first 10 elements of the input list\n",
        "  \"\"\"      \n",
        "  second_list = []\n",
        "  for element in input_list[:10]:\n",
        "    second_list.append(element)\n",
        "  first_list = input_list[10:]\n",
        "  return first_list, second_list\n",
        "\n",
        "\n",
        "def testNet(datasetLength, dataloader, net, matrix, flag):\n",
        "  \"\"\"\n",
        "  This function tests the accuracy of the neural network on the images of the\n",
        "  input dataloader.\n",
        "  \"\"\"\n",
        "  net = net.to(DEVICE)\n",
        "  net.train(False)\n",
        "\n",
        "  running_corrects = 0\n",
        "\n",
        "  for images, labels in dataloader:\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    outputs = net(images)  # Forward Pass\n",
        "    _, preds = torch.max(outputs.data, 1)  # Get predictions\n",
        "\n",
        "    #Debugging purpose, print labels of predictions\n",
        "    ##print(preds)\n",
        "    if flag:\n",
        "      update_confusion_matrix(matrix, preds, labels)\n",
        "\n",
        "    # Update Corrects\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / datasetLength\n",
        "  if flag:\n",
        "    show_confusion_matrix(matrix)\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def accuracy_plot(accuracies):\n",
        "  \"\"\"\n",
        "  This function plots the accuracy chart from 10 points with interpolation\n",
        "  \"\"\"\n",
        "  # FOR MEAN STD PLOT https://stackoverflow.com/questions/22481854/plot-mean-and-standard-deviation\n",
        "\n",
        "  tck,u     = interpolate.splprep( [[i*10 for i in range(1,len(accuracies)+1)],accuracies] ,s = 0 )\n",
        "  xnew,ynew = interpolate.splev( np.linspace( 0, 1, 100 ), tck,der = 0)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(15,14), facecolor='white')\n",
        "\n",
        "  plt.rc('font', size=20)\n",
        "  plt.plot( [i*10 for i in range(1,len(accuracies)+1)],accuracies,'.' , xnew ,ynew, label = \"accuracy\", c='orange' )\n",
        "  ax.set_ylabel(\"Accuracy\")\n",
        "  ax.set_xlabel(\"Classes\")\n",
        "  ax.minorticks_on()\n",
        "  plt.title(\"Accuracies obtained with finetuning of a ResNet network\")\n",
        "  plt.yticks(np.arange(0, 1.1, .1))\n",
        "  plt.xticks(np.arange(0, 110, 10))\n",
        "  plt.grid(axis='y',which='major', linestyle='-', linewidth='0.5', color='black') \n",
        "  plt.grid(axis='y',which='minor', linestyle=':', linewidth='0.5', color='grey')\n",
        "\n",
        "  # Plot also the value of the point close to it\n",
        "  for in_i, in_j in zip([i*10 for i in range(1,len(accuracies)+1)], accuracies):\n",
        "    ax.annotate(str(round(in_j, 3)), xy=(in_i, in_j))\n",
        "\n",
        "  plt.savefig('test.png', format='png', dpi=300)\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JqYHsh6ZHw1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**Arguments**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qCqYt307dm7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 10  # Init at 10 because first train is on 10 classes\n",
        "\n",
        "SEED = 12  # Used for the pseudorandom shuffle of the split\n",
        "\n",
        "BATCH_SIZE = 128  # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                  # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 2  # The initial Learning Rate\n",
        "MOMENTUM = 0.9  # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70  # Total number of training epochs\n",
        "MILESTONES = [48, 62]  # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.2  # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6t7fLFuZBgF",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**Transformations definition**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pFm2Gxyedm71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "                                      transforms.Pad(4), # Add padding\n",
        "                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # https://gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                 \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jigjtgKJYmUU",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**Network Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K2lCILO5dm8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = ResNet18()\n",
        "best_model = ResNet18()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPoIu2qOYCty",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**Train, Validation** *not so fair* **and Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fVxqsJc0dm8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of 100 classes in random order\n",
        "lbls = [i for i in range(0,100)]  \n",
        "random.seed(SEED)\n",
        "random.shuffle(lbls)\n",
        "\n",
        "NUM_GROUPS = 10  # Number of training groups in which to divide the 100 classes\n",
        "\n",
        "test_accuracies = []\n",
        "best_eval_accuracy = 0\n",
        "\n",
        "# Folder where to store CIFAR100 Dataset\n",
        "DATA_DIR = './CIFAR100'\n",
        "\n",
        "# Define test dataset outside in order to increment it, instead of initializing it every cycle iteration\n",
        "test_dataset = CIFAR100_tError(DATA_DIR, train=False, transform=eval_transform, download=True)\n",
        "\n",
        "for i in range(0, NUM_GROUPS): # Iterating on all the groups of classes\n",
        "  matrix = new_confusion_matrix(lenx=(i+1)*10, leny=(i+1)*10)\n",
        "  print('\\n', f'Training network on group {i+1} of {10}', '\\n')\n",
        "\n",
        "  # Every iteration the first 10 classes of the shuffled list of all unseen \n",
        "  # classes are selected to train the network and removed from the shuffled list\n",
        "  lbls, new_labels = make_data_labels(lbls)\n",
        "  \n",
        "  # Training set is the train dataset of exactly 10 new classes\n",
        "  train_dataset = CIFAR100_tError(DATA_DIR, train=True, transform=train_transform, download=True)\n",
        "  train_dataset.increment(new_labels, [j for j in range(0+i*10, 10+i*10)])\n",
        "\n",
        "  # Evaluation set is the subset of the test dataset of the only 10 new classes\n",
        "  # (for this reason -> \"Not so fair\" Validation)\n",
        "  eval_dataset = CIFAR100_tError(DATA_DIR, train=False, transform=eval_transform, download=True)\n",
        "  eval_dataset.increment(new_labels, [j for j in range(0+i*10, 10+i*10)])    \n",
        "    \n",
        "  # Test dataset is the test dataset of the 10 new classes and all the previous\n",
        "  test_dataset.increment(new_labels, [j for j in range(0+i*10, 10+i*10)])\n",
        "\n",
        "  # Define dataloader\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "  eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "  # Prepare Training\n",
        "  criterion = nn.BCEWithLogitsLoss() # for classification: Cross Entropy\n",
        "  parameters_to_optimize = net.parameters()  # Parameters to optimize\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1) \n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "  current_step = 0\n",
        "\n",
        "  # Training\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))\n",
        "    \n",
        "    for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      net.train() # Sets module in training mode\n",
        "      optimizer.zero_grad()  # Zero-ing the gradients\n",
        "      outputs = net(images)  # Forward pass to the network\n",
        "\n",
        "      # One hot encoding labels for binary cross-entropy loss\n",
        "      labels_onehot = nn.functional.one_hot(labels,100)\n",
        "      labels_onehot = labels_onehot.type_as(outputs)\n",
        "\n",
        "      loss = criterion(outputs, labels_onehot)\n",
        "    \n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      loss.backward()  # backward pass: computes gradients\n",
        "      optimizer.step()  # update weights based on accumulated gradients\n",
        "      current_step += 1\n",
        "\n",
        "    # Validation is \"Not so fair\" because it is performed on eval_dataset \n",
        "    # which is a subset of the current 10 new training classes of the\n",
        "    # Test dataset. (Validation is not to be performed on Test data)\n",
        "    eval_accuracy = testNet(float(len(eval_dataset)), eval_dataloader, net, matrix, False)\n",
        "    if eval_accuracy > best_eval_accuracy:\n",
        "      best_eval_accuracy = eval_accuracy\n",
        "      best_model = copy.deepcopy(net)\n",
        "      print('Validation Accuracy: {}'.format(eval_accuracy))\n",
        "\n",
        "    scheduler.step()  # Step the scheduler\n",
        "\n",
        "  # Test Phase (Performed on current 10 new classes and all the previous ones)\n",
        "  test_accuracy = testNet(float(len(test_dataset)), test_dataloader, best_model, matrix, True)\n",
        "  test_accuracies.append(test_accuracy)\n",
        "  print('Test Accuracy: {}'.format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VphrtfF8XqsN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**Prints & Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6Asycu5ddm8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_accuracies)\n",
        "\n",
        "accuracy_plot(test_accuracies)\n",
        "#obtained [0.844, 0.4525, 0.2976666666666667, 0.224, 0.1808, 0.1535, 0.125, 0.113625, 0.10577777777777778, 0.0929]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}