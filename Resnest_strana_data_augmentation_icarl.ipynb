{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Resnest_strana_data_augmentation_icarl.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicordaro/Project_MLDL/blob/master/Resnest_strana_data_augmentation_icarl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "DsKxlaXadm7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "fNgxhFrmdm7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet34\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qCqYt307dm7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "# Init at 10 because first train is on 10 classes\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Used for the pseudorandom shuffle of the split\n",
        "SEED = 12\n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 2         # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70     # Total number of training epochs (iterations over dataset)\n",
        "MILESTONES = [48, 62]  # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.2          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pFm2Gxyedm71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.Pad(4),\n",
        "                                      transforms.RandomCrop(32),# Crops a central square patch of the image\n",
        "                                     # 224 because torchvision's AlexNet needs a 224x224 inpu                           # Remember this when applying different transformations, otherwise you get an er\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # https://gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                 \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fP5vAuX9dm75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "259fc728-c6a4-455c-c907-844d95d8fb3c"
      },
      "source": [
        "# Clone github repository with data\n",
        "# if os.path.isdir('./Project_MLDL'):\n",
        "!rm -rf Project_MLDL\n",
        "if not os.path.isdir('./CIFAR100_tError'):\n",
        "  !git clone https://github.com/Nicordaro/Project_MLDL\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Project_MLDL'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 140 (delta 66), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (140/140), 342.14 KiB | 630.00 KiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fXLgSBpedm8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Project_MLDL.CIFAR100_tError import CIFAR100_tError\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "DATA_DIR = './CIFAR100'\n",
        "\n",
        "lbls = [i for i in range(0,100)]  #Array of classes integer-encoded (?)\n",
        "random.seed(SEED)\n",
        "random.shuffle(lbls)\n",
        "\n",
        "def make_data_labels(lbls):       #After shuffle, take first 10 classes, and remove the first 10 from the list passed as argument\n",
        "    new_labels=[]\n",
        "    for el in lbls[:10]:\n",
        "        new_labels.append(el)\n",
        "    lbls = lbls[10:]\n",
        "\n",
        "    return lbls, new_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pqtzD4ALdm8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "# import torch.nn as nn\n",
        "# import math\n",
        "# import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "# \"\"\"\n",
        "# Credits to @hshustc\n",
        "# Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# def conv3x3(in_planes, out_planes, stride=1):\n",
        "#     \"\"\"3x3 convolution with padding\"\"\"\n",
        "#     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "#                      padding=1, bias=False)\n",
        "\n",
        "# class BasicBlock(nn.Module):\n",
        "#     expansion = 1\n",
        "\n",
        "#     def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "#         super(BasicBlock, self).__init__()\n",
        "#         self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "#         self.bn1 = nn.BatchNorm2d(planes)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.conv2 = conv3x3(planes, planes)\n",
        "#         self.bn2 = nn.BatchNorm2d(planes)\n",
        "#         self.downsample = downsample\n",
        "#         self.stride = stride\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         residual = x\n",
        "\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.bn1(out)\n",
        "#         out = self.relu(out)\n",
        "\n",
        "#         out = self.conv2(out)\n",
        "#         out = self.bn2(out)\n",
        "\n",
        "#         if self.downsample is not None:\n",
        "#             residual = self.downsample(x)\n",
        "\n",
        "#         out += residual\n",
        "#         out = self.relu(out)\n",
        "\n",
        "#         return out\n",
        "\n",
        "\n",
        "# class Bottleneck(nn.Module):\n",
        "#     expansion = 4\n",
        "\n",
        "#     def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "#         super(Bottleneck, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "#         self.bn1 = nn.BatchNorm2d(planes)\n",
        "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "#                                padding=1, bias=False)\n",
        "#         self.bn2 = nn.BatchNorm2d(planes)\n",
        "#         self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "#         self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.downsample = downsample\n",
        "#         self.stride = stride\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         residual = x\n",
        "\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.bn1(out)\n",
        "#         out = self.relu(out)\n",
        "\n",
        "#         out = self.conv2(out)\n",
        "#         out = self.bn2(out)\n",
        "#         out = self.relu(out)\n",
        "\n",
        "#         out = self.conv3(out)\n",
        "#         out = self.bn3(out)\n",
        "\n",
        "#         if self.downsample is not None:\n",
        "#             residual = self.downsample(x)\n",
        "\n",
        "#         out += residual\n",
        "#         out = self.relu(out)\n",
        "\n",
        "#         return out\n",
        "\n",
        "\n",
        "# class ResNet(nn.Module):\n",
        "\n",
        "#     def __init__(self, block, layers, num_classes=100):\n",
        "#         self.inplanes = 16\n",
        "#         super(ResNet, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "#                                bias=False)\n",
        "#         self.bn1 = nn.BatchNorm2d(16)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "#         self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "#         self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "#         self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "#         self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Conv2d):\n",
        "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "#             elif isinstance(m, nn.BatchNorm2d):\n",
        "#                 nn.init.constant_(m.weight, 1)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "\n",
        "#     def _make_layer(self, block, planes, blocks, stride=1):\n",
        "#         downsample = None\n",
        "#         if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "#             downsample = nn.Sequential(\n",
        "#                 nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "#                           kernel_size=1, stride=stride, bias=False),\n",
        "#                 nn.BatchNorm2d(planes * block.expansion),\n",
        "#             )\n",
        "\n",
        "#         layers = []\n",
        "#         layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "#         self.inplanes = planes * block.expansion\n",
        "#         for i in range(1, blocks):\n",
        "#             layers.append(block(self.inplanes, planes))\n",
        "\n",
        "#         return nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.bn1(x)\n",
        "#         x = self.relu(x)\n",
        "\n",
        "#         x = self.layer1(x)\n",
        "#         x = self.layer2(x)\n",
        "#         x = self.layer3(x)\n",
        "\n",
        "#         x = self.avgpool(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.fc(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# def resnet20(pretrained=False, **kwargs):\n",
        "#     n = 3\n",
        "#     model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "#     return model\n",
        "\n",
        "# def resnet32(pretrained=False, **kwargs):\n",
        "#     n = 5\n",
        "#     model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "#     return model\n",
        "\n",
        "# def resnet56(pretrained=False, **kwargs):\n",
        "#     n = 9\n",
        "#     model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "#     return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K2lCILO5dm8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#net = resnet32()\n",
        "#net = ResNet()\n",
        "net = ResNet(BasicBlock,[2,2,2,2],100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fVxqsJc0dm8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2e53c610-a9c8-490b-c56b-fc4aa23ab2d6"
      },
      "source": [
        "import numpy as np\n",
        "BATCH_TO_TEST = 10\n",
        "accuracies = []\n",
        "\n",
        "# Define test dataset outside in order to increment it, instead of initializing it every cycle iteration\n",
        "test_dataset = CIFAR100_tError(DATA_DIR, train=False, transform=eval_transform, download=True)\n",
        "for i in range(0,BATCH_TO_TEST): #one iteration for 10 classes\n",
        "    print(f'Starting training with batch {i+1}')\n",
        "    lbls, new_labels = make_data_labels(lbls)\n",
        "  \n",
        "  # Define Train dataset\n",
        "    train_dataset = CIFAR100_tError(DATA_DIR, train=True, transform=train_transform, download=True)       \n",
        "  \n",
        "  # Increment dataset with new labels mapped with list comprehension\n",
        "    train_dataset.increment(new_labels,[j for j in range(0+i*10,10+i*10)])\n",
        "    test_dataset.increment(new_labels,[j for j in range(0+i*10,10+i*10)])\n",
        "\n",
        "  # Change number of neurons in last fully connected layer\n",
        "    #net.linear = nn.Linear(512, NUM_CLASSES*(i+1))\n",
        "#     if i>1:\n",
        "#         new_block = nn.Linear(512, 10).to(DEVICE)\n",
        "#         new_weights = torch.cat((net.fc.weight.data, new_block.weight.data), dim = 0)\n",
        "#         new_biases = torch.cat((net.fc.bias.data, new_block.bias.data), dim = 0)\n",
        "\n",
        "#         new_layer = nn.Linear(512 , new_weights.size(0))\n",
        "#         net.fc = new_layer\n",
        "#         net.fc.weight.data = new_weights\n",
        "#         net.fc.bias.data = new_biases\n",
        "\n",
        "  # Define dataloader\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "  #prepare training\n",
        "  # Loss function\n",
        "    criterion = nn.BCEWithLogitsLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "  # Parameters to optimize:\n",
        "    parameters_to_optimize = net.parameters()\n",
        "\n",
        "  # Optimizer\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  # Scheduler\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1)\n",
        "\n",
        "  # training \n",
        "  # By default, everything is loaded to cpu\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "    current_step = 0\n",
        "\n",
        "  # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "    # if epoch==5:\n",
        "    #   for g in optimizer.param_groups:\n",
        "    #     g['lr'] = g['lr']/5\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))\n",
        "    \n",
        "    # Iterate over the dataset\n",
        "        for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            net.train() # Sets module in training mode\n",
        "\n",
        "      # PyTorch, by default, accumulates gradients after each backward pass\n",
        "      # We need to manually set the gradients to zero before starting a new iteration\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      # Forward pass to the network\n",
        "            outputs = net(images)\n",
        "      \n",
        "            labels_onehot = nn.functional.one_hot(labels,100)\n",
        "            labels_onehot = labels_onehot.type_as(outputs)\n",
        "\n",
        "            loss = criterion(outputs, labels_onehot)\n",
        "      \n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "            current_step += 1\n",
        "\n",
        "    # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "  #test phase\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "    running_corrects = 0\n",
        "\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "    # Forward Pass\n",
        "        outputs = net(images)\n",
        "\n",
        "    # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    #Debugging purpose, print labels of predictions\n",
        "    ##print(preds)\n",
        "\n",
        "    # Update Corrects\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "  #Store accuracies for plotting purposes\n",
        "    accuracies.append(accuracy)\n",
        "    print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Starting training with batch 1\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6Asycu5ddm8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(accuracies)\n",
        "#obtained [0.844, 0.4525, 0.2976666666666667, 0.224, 0.1808, 0.1535, 0.125, 0.113625, 0.10577777777777778, 0.0929]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}