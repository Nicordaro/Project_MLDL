{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "kernel83f8e56162.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicordaro/Project_MLDL/blob/master/point4/svm_classifier_icarl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fP5vAuX9dm75",
        "outputId": "13c908b5-64f4-4f83-a47a-3b4c5aa6c5fb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "import os\n",
        "# Clone github repository with data\n",
        "# if os.path.isdir('./Project_MLDL'):\n",
        "!rm -rf Project_MLDL\n",
        "if not os.path.isdir('./CIFAR100_tError'):\n",
        "  !git clone https://github.com/Nicordaro/Project_MLDL\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Project_MLDL'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/71)\u001b[K\rremote: Counting objects:   2% (2/71)\u001b[K\rremote: Counting objects:   4% (3/71)\u001b[K\rremote: Counting objects:   5% (4/71)\u001b[K\rremote: Counting objects:   7% (5/71)\u001b[K\rremote: Counting objects:   8% (6/71)\u001b[K\rremote: Counting objects:   9% (7/71)\u001b[K\rremote: Counting objects:  11% (8/71)\u001b[K\rremote: Counting objects:  12% (9/71)\u001b[K\rremote: Counting objects:  14% (10/71)\u001b[K\rremote: Counting objects:  15% (11/71)\u001b[K\rremote: Counting objects:  16% (12/71)\u001b[K\rremote: Counting objects:  18% (13/71)\u001b[K\rremote: Counting objects:  19% (14/71)\u001b[K\rremote: Counting objects:  21% (15/71)\u001b[K\rremote: Counting objects:  22% (16/71)\u001b[K\rremote: Counting objects:  23% (17/71)\u001b[K\rremote: Counting objects:  25% (18/71)\u001b[K\rremote: Counting objects:  26% (19/71)\u001b[K\rremote: Counting objects:  28% (20/71)\u001b[K\rremote: Counting objects:  29% (21/71)\u001b[K\rremote: Counting objects:  30% (22/71)\u001b[K\rremote: Counting objects:  32% (23/71)\u001b[K\rremote: Counting objects:  33% (24/71)\u001b[K\rremote: Counting objects:  35% (25/71)\u001b[K\rremote: Counting objects:  36% (26/71)\u001b[K\rremote: Counting objects:  38% (27/71)\u001b[K\rremote: Counting objects:  39% (28/71)\u001b[K\rremote: Counting objects:  40% (29/71)\u001b[K\rremote: Counting objects:  42% (30/71)\u001b[K\rremote: Counting objects:  43% (31/71)\u001b[K\rremote: Counting objects:  45% (32/71)\u001b[K\rremote: Counting objects:  46% (33/71)\u001b[K\rremote: Counting objects:  47% (34/71)\u001b[K\rremote: Counting objects:  49% (35/71)\u001b[K\rremote: Counting objects:  50% (36/71)\u001b[K\rremote: Counting objects:  52% (37/71)\u001b[K\rremote: Counting objects:  53% (38/71)\u001b[K\rremote: Counting objects:  54% (39/71)\u001b[K\rremote: Counting objects:  56% (40/71)\u001b[K\rremote: Counting objects:  57% (41/71)\u001b[K\rremote: Counting objects:  59% (42/71)\u001b[K\rremote: Counting objects:  60% (43/71)\u001b[K\rremote: Counting objects:  61% (44/71)\u001b[K\rremote: Counting objects:  63% (45/71)\u001b[K\rremote: Counting objects:  64% (46/71)\u001b[K\rremote: Counting objects:  66% (47/71)\u001b[K\rremote: Counting objects:  67% (48/71)\u001b[K\rremote: Counting objects:  69% (49/71)\u001b[K\rremote: Counting objects:  70% (50/71)\u001b[K\rremote: Counting objects:  71% (51/71)\u001b[K\rremote: Counting objects:  73% (52/71)\u001b[K\rremote: Counting objects:  74% (53/71)\u001b[K\rremote: Counting objects:  76% (54/71)\u001b[K\rremote: Counting objects:  77% (55/71)\u001b[K\rremote: Counting objects:  78% (56/71)\u001b[K\rremote: Counting objects:  80% (57/71)\u001b[K\rremote: Counting objects:  81% (58/71)\u001b[K\rremote: Counting objects:  83% (59/71)\u001b[K\rremote: Counting objects:  84% (60/71)\u001b[K\rremote: Counting objects:  85% (61/71)\u001b[K\rremote: Counting objects:  87% (62/71)\u001b[K\rremote: Counting objects:  88% (63/71)\u001b[K\rremote: Counting objects:  90% (64/71)\u001b[K\rremote: Counting objects:  91% (65/71)\u001b[K\rremote: Counting objects:  92% (66/71)\u001b[K\rremote: Counting objects:  94% (67/71)\u001b[K\rremote: Counting objects:  95% (68/71)\u001b[K\rremote: Counting objects:  97% (69/71)\u001b[K\rremote: Counting objects:  98% (70/71)\u001b[K\rremote: Counting objects: 100% (71/71)\u001b[K\rremote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/71)\u001b[K\rremote: Compressing objects:   2% (2/71)\u001b[K\rremote: Compressing objects:   4% (3/71)\u001b[K\rremote: Compressing objects:   5% (4/71)\u001b[K\rremote: Compressing objects:   7% (5/71)\u001b[K\rremote: Compressing objects:   8% (6/71)\u001b[K\rremote: Compressing objects:   9% (7/71)\u001b[K\rremote: Compressing objects:  11% (8/71)\u001b[K\rremote: Compressing objects:  12% (9/71)\u001b[K\rremote: Compressing objects:  14% (10/71)\u001b[K\rremote: Compressing objects:  15% (11/71)\u001b[K\rremote: Compressing objects:  16% (12/71)\u001b[K\rremote: Compressing objects:  18% (13/71)\u001b[K\rremote: Compressing objects:  19% (14/71)\u001b[K\rremote: Compressing objects:  21% (15/71)\u001b[K\rremote: Compressing objects:  22% (16/71)\u001b[K\rremote: Compressing objects:  23% (17/71)\u001b[K\rremote: Compressing objects:  25% (18/71)\u001b[K\rremote: Compressing objects:  26% (19/71)\u001b[K\rremote: Compressing objects:  28% (20/71)\u001b[K\rremote: Compressing objects:  29% (21/71)\u001b[K\rremote: Compressing objects:  30% (22/71)\u001b[K\rremote: Compressing objects:  32% (23/71)\u001b[K\rremote: Compressing objects:  33% (24/71)\u001b[K\rremote: Compressing objects:  35% (25/71)\u001b[K\rremote: Compressing objects:  36% (26/71)\u001b[K\rremote: Compressing objects:  38% (27/71)\u001b[K\rremote: Compressing objects:  39% (28/71)\u001b[K\rremote: Compressing objects:  40% (29/71)\u001b[K\rremote: Compressing objects:  42% (30/71)\u001b[K\rremote: Compressing objects:  43% (31/71)\u001b[K\rremote: Compressing objects:  45% (32/71)\u001b[K\rremote: Compressing objects:  46% (33/71)\u001b[K\rremote: Compressing objects:  47% (34/71)\u001b[K\rremote: Compressing objects:  49% (35/71)\u001b[K\rremote: Compressing objects:  50% (36/71)\u001b[K\rremote: Compressing objects:  52% (37/71)\u001b[K\rremote: Compressing objects:  53% (38/71)\u001b[K\rremote: Compressing objects:  54% (39/71)\u001b[K\rremote: Compressing objects:  56% (40/71)\u001b[K\rremote: Compressing objects:  57% (41/71)\u001b[K\rremote: Compressing objects:  59% (42/71)\u001b[K\rremote: Compressing objects:  60% (43/71)\u001b[K\rremote: Compressing objects:  61% (44/71)\u001b[K\rremote: Compressing objects:  63% (45/71)\u001b[K\rremote: Compressing objects:  64% (46/71)\u001b[K\rremote: Compressing objects:  66% (47/71)\u001b[K\rremote: Compressing objects:  67% (48/71)\u001b[K\rremote: Compressing objects:  69% (49/71)\u001b[K\rremote: Compressing objects:  70% (50/71)\u001b[K\rremote: Compressing objects:  71% (51/71)\u001b[K\rremote: Compressing objects:  73% (52/71)\u001b[K\rremote: Compressing objects:  74% (53/71)\u001b[K\rremote: Compressing objects:  76% (54/71)\u001b[K\rremote: Compressing objects:  77% (55/71)\u001b[K\rremote: Compressing objects:  78% (56/71)\u001b[K\rremote: Compressing objects:  80% (57/71)\u001b[K\rremote: Compressing objects:  81% (58/71)\u001b[K\rremote: Compressing objects:  83% (59/71)\u001b[K\rremote: Compressing objects:  84% (60/71)\u001b[K\rremote: Compressing objects:  85% (61/71)\u001b[K\rremote: Compressing objects:  87% (62/71)\u001b[K\rremote: Compressing objects:  88% (63/71)\u001b[K\rremote: Compressing objects:  90% (64/71)\u001b[K\rremote: Compressing objects:  91% (65/71)\u001b[K\rremote: Compressing objects:  92% (66/71)\u001b[K\rremote: Compressing objects:  94% (67/71)\u001b[K\rremote: Compressing objects:  95% (68/71)\u001b[K\rremote: Compressing objects:  97% (69/71)\u001b[K\rremote: Compressing objects:  98% (70/71)\u001b[K\rremote: Compressing objects: 100% (71/71)\u001b[K\rremote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "Receiving objects:   0% (1/500)   \rReceiving objects:   1% (5/500)   \rReceiving objects:   2% (10/500)   \rReceiving objects:   3% (15/500)   \rReceiving objects:   4% (20/500)   \rReceiving objects:   5% (25/500)   \rReceiving objects:   6% (30/500)   \rReceiving objects:   7% (35/500)   \rReceiving objects:   8% (40/500)   \rReceiving objects:   9% (45/500)   \rReceiving objects:  10% (50/500)   \rReceiving objects:  11% (55/500)   \rReceiving objects:  12% (60/500)   \rReceiving objects:  13% (65/500)   \rReceiving objects:  14% (70/500)   \rReceiving objects:  15% (75/500)   \rReceiving objects:  16% (80/500)   \rReceiving objects:  17% (85/500)   \rReceiving objects:  18% (90/500)   \rReceiving objects:  19% (95/500)   \rReceiving objects:  20% (100/500)   \rReceiving objects:  21% (105/500)   \rReceiving objects:  22% (110/500)   \rReceiving objects:  23% (115/500)   \rReceiving objects:  24% (120/500)   \rReceiving objects:  25% (125/500)   \rReceiving objects:  26% (130/500)   \rReceiving objects:  27% (135/500)   \rReceiving objects:  28% (140/500)   \rReceiving objects:  29% (145/500)   \rReceiving objects:  30% (150/500)   \rReceiving objects:  31% (155/500)   \rReceiving objects:  32% (160/500)   \rReceiving objects:  33% (165/500)   \rReceiving objects:  34% (170/500)   \rReceiving objects:  35% (175/500)   \rReceiving objects:  36% (180/500)   \rReceiving objects:  37% (185/500)   \rReceiving objects:  38% (190/500)   \rReceiving objects:  39% (195/500)   \rReceiving objects:  40% (200/500)   \rReceiving objects:  41% (205/500)   \rReceiving objects:  42% (210/500)   \rReceiving objects:  43% (215/500)   \rReceiving objects:  44% (220/500)   \rReceiving objects:  45% (225/500)   \rReceiving objects:  46% (230/500)   \rReceiving objects:  47% (235/500)   \rReceiving objects:  48% (240/500)   \rReceiving objects:  49% (245/500)   \rReceiving objects:  50% (250/500)   \rReceiving objects:  51% (255/500)   \rReceiving objects:  52% (260/500)   \rReceiving objects:  53% (265/500)   \rReceiving objects:  54% (270/500)   \rReceiving objects:  55% (275/500)   \rReceiving objects:  56% (280/500)   \rReceiving objects:  57% (285/500)   \rReceiving objects:  58% (290/500)   \rReceiving objects:  59% (295/500)   \rReceiving objects:  60% (300/500)   \rReceiving objects:  61% (305/500)   \rReceiving objects:  62% (310/500)   \rReceiving objects:  63% (315/500)   \rReceiving objects:  64% (320/500)   \rReceiving objects:  65% (325/500)   \rReceiving objects:  66% (330/500)   \rReceiving objects:  67% (335/500)   \rReceiving objects:  68% (340/500)   \rReceiving objects:  69% (345/500)   \rReceiving objects:  70% (350/500)   \rReceiving objects:  71% (355/500)   \rReceiving objects:  72% (360/500)   \rReceiving objects:  73% (365/500)   \rReceiving objects:  74% (370/500)   \rReceiving objects:  75% (375/500)   \rReceiving objects:  76% (380/500)   \rReceiving objects:  77% (385/500)   \rReceiving objects:  78% (390/500)   \rReceiving objects:  79% (395/500)   \rReceiving objects:  80% (400/500)   \rReceiving objects:  81% (405/500)   \rReceiving objects:  82% (410/500)   \rReceiving objects:  83% (415/500)   \rReceiving objects:  84% (420/500)   \rReceiving objects:  85% (425/500)   \rReceiving objects:  86% (430/500)   \rReceiving objects:  87% (435/500)   \rReceiving objects:  88% (440/500)   \rReceiving objects:  89% (445/500)   \rReceiving objects:  90% (450/500)   \rReceiving objects:  91% (455/500)   \rReceiving objects:  92% (460/500)   \rReceiving objects:  93% (465/500)   \rReceiving objects:  94% (470/500)   \rReceiving objects:  95% (475/500)   \rremote: Total 500 (delta 36), reused 0 (delta 0), pack-reused 429\u001b[K\n",
            "Receiving objects:  96% (480/500)   \rReceiving objects:  97% (485/500)   \rReceiving objects:  98% (490/500)   \rReceiving objects:  99% (495/500)   \rReceiving objects: 100% (500/500)   \rReceiving objects: 100% (500/500), 1.30 MiB | 24.23 MiB/s, done.\n",
            "Resolving deltas:   0% (0/274)   \rResolving deltas:  14% (41/274)   \rResolving deltas:  15% (43/274)   \rResolving deltas:  16% (46/274)   \rResolving deltas:  20% (57/274)   \rResolving deltas:  21% (59/274)   \rResolving deltas:  23% (65/274)   \rResolving deltas:  43% (118/274)   \rResolving deltas:  56% (155/274)   \rResolving deltas:  57% (157/274)   \rResolving deltas:  62% (171/274)   \rResolving deltas:  69% (190/274)   \rResolving deltas:  70% (192/274)   \rResolving deltas:  81% (224/274)   \rResolving deltas:  82% (226/274)   \rResolving deltas:  83% (228/274)   \rResolving deltas:  84% (232/274)   \rResolving deltas:  85% (234/274)   \rResolving deltas:  93% (255/274)   \rResolving deltas:  97% (266/274)   \rResolving deltas:  99% (272/274)   \rResolving deltas: 100% (274/274)   \rResolving deltas: 100% (274/274), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqazEUOdZpEb",
        "colab_type": "text"
      },
      "source": [
        "**Imports**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "fNgxhFrmdm7d",
        "outputId": "c7545de8-0ae5-4dbd-db7b-ee48f0e6732b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import gc\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image\n",
        "from Project_MLDL.confusion_matrix import *\n",
        "from Project_MLDL.CIFAR100_tError import CIFAR100_tError\n",
        "from Project_MLDL.model_finetuning import ResNet18\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.svm import SVC\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet34\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Garbage collector\n",
        "gc.enable()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JqYHsh6ZHw1",
        "colab_type": "text"
      },
      "source": [
        "**Arguments**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qCqYt307dm7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "# Init at 10 because first train is on 10 classes\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Used for the pseudorandom shuffle of the split\n",
        "SEED = 12\n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 2         # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70     # Total number of training epochs (iterations over dataset)\n",
        "MILESTONES = [48, 62]  # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.2          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6t7fLFuZBgF",
        "colab_type": "text"
      },
      "source": [
        "**Transformations definition**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pFm2Gxyedm71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "                                      transforms.Pad(4), # Add padding\n",
        "                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) \n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                 \n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                 \n",
        "])\n",
        "exemplar_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "                                      transforms.Pad(4), # Add padding\n",
        "                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "                                      transforms.ToTensor() # Turn PIL Image to torch.Tensor\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW30kNv9jlEL",
        "colab_type": "text"
      },
      "source": [
        "*New label function*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fXLgSBpedm8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = './CIFAR100'\n",
        "\n",
        "lbls = [i for i in range(0,100)]  #Array of classes integer-encoded (?)\n",
        "random.seed(SEED)\n",
        "random.shuffle(lbls)\n",
        "\n",
        "def make_data_labels(lbls):     #After shuffle, take first 10 classes, and remove the first 10 from the list passed as argument\n",
        "    new_labels=[]\n",
        "    for el in lbls[:10]:\n",
        "        new_labels.append(el)\n",
        "    lbls = lbls[10:]\n",
        "\n",
        "    return lbls, new_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8rsulwaFi8_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\"\"\"\n",
        "Take 2 datasets and combine them\n",
        "\"\"\"\n",
        "class joint_dataset(Dataset):\n",
        "    \n",
        "    def __init__(self,d1,d2):\n",
        "        self.l1 = len(d1)\n",
        "        self.l2 = len(d2)\n",
        "        self.d1 = d1\n",
        "        self.d2 = d2\n",
        "    def __getitem__(self,index):\n",
        "        if index>=self.l1:\n",
        "            image,label = self.d2[index-self.l1]\n",
        "            image = exemplar_transform(image) # Apply transformations to images\n",
        "            return image,label\n",
        "        else:\n",
        "            image,label = self.d1[index]\n",
        "            return image,label\n",
        "    def __len__(self):\n",
        "        return (self.l1 + self.l2)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmirquleWTnE",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class iCaRL(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(iCaRL, self).__init__()\n",
        "\n",
        "    # CNN with 100 neurons in the last layer (Features Extractor + FC)\n",
        "    self.net = ResNet18()\n",
        "    self.net.linear = nn.Linear(512, 100)\n",
        "    \n",
        "    # Classification and Distillation losses\n",
        "    self.cls_loss = nn.BCEWithLogitsLoss()\n",
        "    self.dist_loss = nn.BCELoss()\n",
        "    \n",
        "    self.exemplar_sets= [] # List of exemplar sets, one for each class\n",
        "    self.exemplars_means= [] # List of exemplar means, one for each exemplar set\n",
        "    self.svm_exemplars_features = []\n",
        "    self.svm_exemplars_labels = []\n",
        "\n",
        "    self.clf = SVC()\n",
        "\n",
        "  def classify(self, x):\n",
        "\n",
        "    X = []\n",
        "    clf = self.clf\n",
        "    \n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    feature_extractor = self.net.to(DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "    \n",
        "    feature_x = feature_extractor.extract_features(x) # (batch_size, num_features)\n",
        "    \n",
        "    for features in feature_x.tolist():\n",
        "      X.append(features)\n",
        "      \n",
        "    labels = clf.predict(X)\n",
        "    \n",
        "    \n",
        "    #clean memory\n",
        "    del feature_x\n",
        "    # del means\n",
        "    del x\n",
        "    del feature_extractor\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return labels\n",
        "\n",
        "  def compute_means(self,X):\n",
        "    torch.no_grad()  \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Given the exemplar sets, compute the means\n",
        "\n",
        "    exemplar_means = []\n",
        "    feature_extractor = self.net.to(DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # Don't take the last 10 classes\n",
        "      for i,Py in enumerate(self.exemplar_sets):\n",
        "        if i>=(len(self.exemplar_sets)-10):\n",
        "          break\n",
        "        features=[]\n",
        "        for p in Py:\n",
        "          p = p.to(DEVICE)\n",
        "          feature = feature_extractor.extract_features(p)\n",
        "          features.append(feature)\n",
        "          #Clean Memory\n",
        "          del p\n",
        "          del feature\n",
        "          torch.no_grad()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        features = torch.stack(features) # (num_exemplars,num_features)\n",
        "        mu_y = features.mean(0) \n",
        "        mu_y.data = mu_y.data / mu_y.data.norm() # Normalize\n",
        "        mu_y = mu_y.to('cpu')\n",
        "        exemplar_means.append(mu_y)\n",
        "        del features\n",
        "        del mu_y\n",
        "        torch.no_grad()  \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    #Compute the mean for the last 10 classes exemplars + dataset\n",
        "    with torch.no_grad():\n",
        "      exemplar_sets = self.exemplar_sets[-10:]\n",
        "      for Py,x in zip(exemplar_sets,X):\n",
        "        features=[]\n",
        "        for p in Py:\n",
        "          p = p.to(DEVICE)\n",
        "          feature = feature_extractor.extract_features(p)\n",
        "          features.append(feature)\n",
        "          #Clean Memory\n",
        "          del p\n",
        "          del feature\n",
        "          torch.no_grad()\n",
        "          torch.cuda.empty_cache()\n",
        "        for element in x:\n",
        "          image,label = element\n",
        "          image = image.unsqueeze(dim=0).to(DEVICE)\n",
        "          feature = feature_extractor.extract_features(image)\n",
        "          features.append(feature)\n",
        "          #Clean Memory\n",
        "          del image\n",
        "          del feature\n",
        "          torch.no_grad()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        features = torch.stack(features) # (num_exemplars,num_features)\n",
        "        mu_y = features.mean(0) \n",
        "        mu_y.data = mu_y.data / mu_y.data.norm() # Normalize\n",
        "        mu_y = mu_y.to('cpu')\n",
        "        exemplar_means.append(mu_y)\n",
        "        del features\n",
        "        del mu_y\n",
        "        torch.no_grad()  \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    self.exemplar_means = exemplar_means\n",
        "\n",
        "    #Clean Memory\n",
        "    del exemplar_means\n",
        "    del feature_extractor\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "  def reduce_exemplar_sets(self,m):\n",
        "    for y, P_y in enumerate(self.exemplar_sets):\n",
        "      m = int(m)\n",
        "      self.exemplar_sets[y] = P_y[:m] # keep only the first m exemplars\n",
        "  \n",
        "\n",
        "  def construct_exemplar_set(self,X,m):\n",
        "    \n",
        "    # X dataset containing all elements of class y\n",
        "\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    feature_extractor = self.net.to(DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "    loader = DataLoader(X,batch_size=BATCH_SIZE,shuffle=True,drop_last=False,num_workers = 4) # create dataloader\n",
        "\n",
        "    features = []\n",
        "    \n",
        "    for images,labels in loader:\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      feature = feature_extractor.extract_features(images)\n",
        "      features.append(feature)\n",
        "      del images\n",
        "      del labels\n",
        "      del feature\n",
        "        \n",
        "    features_stacked = torch.cat(features) # (num_elements,num_features)\n",
        "    mean = features_stacked.mean(0) #(num_features)\n",
        "    mean = torch.stack([mean]*features_stacked.size()[0])  # (num_elements,num_features)\n",
        "    torch.cuda.empty_cache()\n",
        "    P = [] # exemplar set\n",
        "    extracted_features = []\n",
        "\n",
        "    summa = torch.zeros(1,features_stacked.size()[1]).to(DEVICE) #(1,num_features)\n",
        "    \n",
        "    for k in range(1,int(m+1)):\n",
        "      s = torch.cat([summa]*features_stacked.size()[0]) #(num_elements,num_features)\n",
        "      index = torch.argmin((mean-(1/k)*(features_stacked+s)).pow(2).sum(1),dim=0)\n",
        "      features_stacked[index] = features_stacked[index]*100\n",
        "      pk = X[index.item()][0].unsqueeze(dim=0) # take the image, leave the label\n",
        "      P.append(pk)\n",
        "      phipk =  feature_extractor.extract_features(pk.to(DEVICE))\n",
        "      summa = summa + phipk # update sum of features\n",
        "      del pk \n",
        "   \n",
        "    #Clear Memory\n",
        "    del summa\n",
        "    del features\n",
        "    del features_stacked\n",
        "    del mean\n",
        "    del s\n",
        "    del feature_extractor\n",
        "    del loader\n",
        "    torch.cuda.empty_cache()\n",
        "    self.exemplar_sets.append(P)\n",
        "    \n",
        "\n",
        "  def update_representation(self,X): \n",
        "    \n",
        "    trans = transforms.ToPILImage()\n",
        "    #Take exemplars\n",
        "    exemplars_dataset = []\n",
        "    for i in range(0,len(self.exemplar_sets)):\n",
        "        for exemplar in self.exemplar_sets[i]:\n",
        "            exemplar = trans(exemplar.squeeze()).convert(\"RGB\")\n",
        "            exemplars_dataset.append((exemplar,i)) #append the image in the list with label = index\n",
        "\n",
        "    D = joint_dataset(X,exemplars_dataset) # Build the dataset \n",
        "   \n",
        "    net = self.net\n",
        "\n",
        "    #optimizer\n",
        "    optimizer = optim.SGD(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
        "\n",
        "    # Scheduler\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1)\n",
        "\n",
        "    criterion = self.cls_loss\n",
        "    criterion2 = self.dist_loss\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "    current_step = 0\n",
        "    net = net.to(DEVICE)\n",
        "    \n",
        "    # Create Dataloader \n",
        "    loader = DataLoader(D, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "    \n",
        "    if len(self.exemplar_sets)>1:\n",
        "        old = copy.deepcopy(net) #copy network before training\n",
        "    \n",
        "    #Training\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))\n",
        "\n",
        "        # Iterate over the dataset\n",
        "      for images, labels in loader:\n",
        "          # Bring data over the device of choice\n",
        "          images = images.to(DEVICE)\n",
        "          labels = labels.to(DEVICE)\n",
        "          net.train()\n",
        "        \n",
        "          optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "          # Forward pass to the network\n",
        "          outputs = net(images)\n",
        "\n",
        "          # One hot encoding labels for binary cross-entropy loss\n",
        "          labels_onehot = nn.functional.one_hot(labels,100).type_as(outputs)\n",
        "\n",
        "          # Compute Loss \n",
        "          if len(self.exemplar_sets)==0:\n",
        "            loss = criterion(outputs, labels_onehot)\n",
        "          else:\n",
        "            labels_onehot = labels_onehot.type_as(outputs)[:,len(self.exemplar_sets):]\n",
        "            out_old = Variable(torch.sigmoid(old(images))[:,:len(self.exemplar_sets)],requires_grad = False)\n",
        "        \n",
        "            target = torch.cat((out_old,labels_onehot),dim=1)\n",
        "            loss = criterion(outputs,target)\n",
        "\n",
        "          if current_step % LOG_FREQUENCY == 0:\n",
        "              print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "          loss.backward()  # backward pass: computes gradients\n",
        "          optimizer.step() # update weights based on accumulated gradients\n",
        "          current_step += 1\n",
        "        \n",
        "      scheduler.step()\n",
        "\n",
        "    # Train SVM\n",
        "    hype_param = {\n",
        "        \"C\": [1, 100, 1000],\n",
        "        \"kernel\": [\"linear\", \"rbf\"]\n",
        "    }\n",
        "\n",
        "    X = self.svm_exemplars_features\n",
        "    y = self.svm_exemplars_labels\n",
        "\n",
        "    # Creazione Features e labels\n",
        "    feature_extractor = net.to(DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "    for images, labels in loader:\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      feature_x = feature_extractor.extract_features(images)\n",
        "      X = X + feature_x.tolist()\n",
        "      y = y + labels.tolist()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    best_config = {}\n",
        "    max_accuracy = 0\n",
        "\n",
        "    for config in ParameterGrid(hype_param):\n",
        "      clf = SVC(**config)\n",
        "      clf.fit(X_train, y_train)\n",
        "      y_test_pred = clf.predict(X_test)\n",
        "      clf_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "      if clf_accuracy > max_accuracy:\n",
        "        max_accuracy = clf_accuracy\n",
        "        print(f\"-----> Score: {clf_accuracy}, config:{config}\")\n",
        "        best_config = config\n",
        "\n",
        "    # Shuffle data (without losing connection between label and feature)\n",
        "    c = list(zip(X, y))\n",
        "    random.shuffle(c)\n",
        "    X, y = zip(*c)\n",
        "\n",
        "    clf = SVC(**best_config)\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    self.svm_exemplars_features = [X[i] for i in clf.support_]\n",
        "    self.svm_exemplars_labels = [y[i] for i in clf.support_]\n",
        "    print(f\"Number of SVM-exemplars: {len(self.svm_exemplars_features)}\")\n",
        "\n",
        "    c = list(zip(self.svm_exemplars_features, self.svm_exemplars_labels))\n",
        "    random.shuffle(c)\n",
        "    self.svm_exemplars_features, self.svm_exemplars_labels = zip(*c)\n",
        "\n",
        "    self.clf = clf\n",
        "    \n",
        "    # Save the trained network and update features extractor\n",
        "    self.net = copy.deepcopy(net)\n",
        "    del net\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfFKyVwGi8pr",
        "trusted": true,
        "outputId": "0b6e2105-a569-43d4-a49c-6dc597227b4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net = iCaRL()\n",
        "K = 2000 # Memory Size\n",
        "t = 0 # Number of classes\n",
        "accuracies = [] # List of results \n",
        "test_dataset = CIFAR100_tError(DATA_DIR, train=False, transform=eval_transform, download=True)\n",
        "\n",
        "for i in range(0,10): # batches of 10\n",
        "  print(f\"processing batch {i+1}\")\n",
        "  #Create Datasets\n",
        "  train_datasets = []\n",
        "  train_dataset_big = CIFAR100_tError(DATA_DIR, train=True, transform=train_transform, download=True)\n",
        "  lbls, new_labels = make_data_labels(lbls) # take 10 new classes\n",
        "  for label in new_labels:\n",
        "    train_dataset = CIFAR100_tError(DATA_DIR, train=True, transform=eval_transform, download=True)\n",
        "    train_dataset.increment([label],[t])\n",
        "    test_dataset.increment([label],[t])\n",
        "    train_dataset_big.increment([label],[t])\n",
        "    train_datasets.append(train_dataset) # List of training examples in per class sets\n",
        "    t += 1\n",
        "  test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
        "  net.update_representation(train_dataset_big)\n",
        "  m = K/t #numbers of exemplars per class\n",
        "  matrix = new_confusion_matrix(lenx=t, leny=t)\n",
        "  net.reduce_exemplar_sets(m)\n",
        "  for X in train_datasets:\n",
        "    print(\"Construct Exemplar\")\n",
        "    net.construct_exemplar_set(X,m) #new exemplar sets\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "  # Test on Test set\n",
        "  running_corrects = 0\n",
        "  print(\"Computing Means\")\n",
        "  net.compute_means(train_datasets)\n",
        "  print(\"classifing\")\n",
        "  for images,labels in test_dataloader:\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "    preds = net.classify(images)\n",
        "    preds = torch.FloatTensor(preds).to(DEVICE)\n",
        "    # update_confusion_matrix(matrix, preds, labels)  # --> da aggiustare se si vuole usare\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "    accuracy = running_corrects / float(len(test_dataset))\n",
        "    del images\n",
        "    del labels\n",
        "    del preds\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "  accuracies.append(accuracy)\n",
        "  print(f\"Test Accuracy: {accuracy}\")\n",
        "  # show_confusion_matrix(matrix)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "processing batch 1\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Starting epoch 1/70, LR = [2]\n",
            "Step 0, Loss 0.7288549542427063\n",
            "Starting epoch 2/70, LR = [2]\n",
            "Step 50, Loss 0.023491621017456055\n",
            "Starting epoch 3/70, LR = [2]\n",
            "Step 100, Loss 0.0231303833425045\n",
            "Starting epoch 4/70, LR = [2]\n",
            "Step 150, Loss 0.020092565566301346\n",
            "Starting epoch 5/70, LR = [2]\n",
            "Starting epoch 6/70, LR = [2]\n",
            "Step 200, Loss 0.01796354167163372\n",
            "Starting epoch 7/70, LR = [2]\n",
            "Step 250, Loss 0.019804371520876884\n",
            "Starting epoch 8/70, LR = [2]\n",
            "Step 300, Loss 0.015494615770876408\n",
            "Starting epoch 9/70, LR = [2]\n",
            "Step 350, Loss 0.016794156283140182\n",
            "Starting epoch 10/70, LR = [2]\n",
            "Starting epoch 11/70, LR = [2]\n",
            "Step 400, Loss 0.013667340390384197\n",
            "Starting epoch 12/70, LR = [2]\n",
            "Step 450, Loss 0.013750087469816208\n",
            "Starting epoch 13/70, LR = [2]\n",
            "Step 500, Loss 0.012991294264793396\n",
            "Starting epoch 14/70, LR = [2]\n",
            "Starting epoch 15/70, LR = [2]\n",
            "Step 550, Loss 0.010888848453760147\n",
            "Starting epoch 16/70, LR = [2]\n",
            "Step 600, Loss 0.013483105227351189\n",
            "Starting epoch 17/70, LR = [2]\n",
            "Step 650, Loss 0.013152767904102802\n",
            "Starting epoch 18/70, LR = [2]\n",
            "Step 700, Loss 0.010727707296609879\n",
            "Starting epoch 19/70, LR = [2]\n",
            "Starting epoch 20/70, LR = [2]\n",
            "Step 750, Loss 0.011458460241556168\n",
            "Starting epoch 21/70, LR = [2]\n",
            "Step 800, Loss 0.008657742291688919\n",
            "Starting epoch 22/70, LR = [2]\n",
            "Step 850, Loss 0.00957824569195509\n",
            "Starting epoch 23/70, LR = [2]\n",
            "Starting epoch 24/70, LR = [2]\n",
            "Step 900, Loss 0.009653124958276749\n",
            "Starting epoch 25/70, LR = [2]\n",
            "Step 950, Loss 0.007360864896327257\n",
            "Starting epoch 26/70, LR = [2]\n",
            "Step 1000, Loss 0.007954645901918411\n",
            "Starting epoch 27/70, LR = [2]\n",
            "Step 1050, Loss 0.012720040045678616\n",
            "Starting epoch 28/70, LR = [2]\n",
            "Starting epoch 29/70, LR = [2]\n",
            "Step 1100, Loss 0.007873964495956898\n",
            "Starting epoch 30/70, LR = [2]\n",
            "Step 1150, Loss 0.007791889365762472\n",
            "Starting epoch 31/70, LR = [2]\n",
            "Step 1200, Loss 0.007034650072455406\n",
            "Starting epoch 32/70, LR = [2]\n",
            "Starting epoch 33/70, LR = [2]\n",
            "Step 1250, Loss 0.0054074618965387344\n",
            "Starting epoch 34/70, LR = [2]\n",
            "Step 1300, Loss 0.007128470577299595\n",
            "Starting epoch 35/70, LR = [2]\n",
            "Step 1350, Loss 0.0061659119091928005\n",
            "Starting epoch 36/70, LR = [2]\n",
            "Step 1400, Loss 0.004479315131902695\n",
            "Starting epoch 37/70, LR = [2]\n",
            "Starting epoch 38/70, LR = [2]\n",
            "Step 1450, Loss 0.0039032751228660345\n",
            "Starting epoch 39/70, LR = [2]\n",
            "Step 1500, Loss 0.0037815573159605265\n",
            "Starting epoch 40/70, LR = [2]\n",
            "Step 1550, Loss 0.0030428499449044466\n",
            "Starting epoch 41/70, LR = [2]\n",
            "Starting epoch 42/70, LR = [2]\n",
            "Step 1600, Loss 0.005390550009906292\n",
            "Starting epoch 43/70, LR = [2]\n",
            "Step 1650, Loss 0.00536302337422967\n",
            "Starting epoch 44/70, LR = [2]\n",
            "Step 1700, Loss 0.004825517535209656\n",
            "Starting epoch 45/70, LR = [2]\n",
            "Step 1750, Loss 0.002664338331669569\n",
            "Starting epoch 46/70, LR = [2]\n",
            "Starting epoch 47/70, LR = [2]\n",
            "Step 1800, Loss 0.0035294662229716778\n",
            "Starting epoch 48/70, LR = [2]\n",
            "Step 1850, Loss 0.0051335678435862064\n",
            "Starting epoch 49/70, LR = [0.4]\n",
            "Step 1900, Loss 0.0010908301919698715\n",
            "Starting epoch 50/70, LR = [0.4]\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "Step 1950, Loss 0.0008507577003911138\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "Step 2000, Loss 0.0007487331167794764\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "Step 2050, Loss 0.0004793931439053267\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "Step 2100, Loss 0.0010770908556878567\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "Step 2150, Loss 0.0004834466380998492\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "Step 2200, Loss 0.00035106632276438177\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "Step 2250, Loss 0.0007155167986638844\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "Step 2300, Loss 0.0006377133540809155\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "Step 2350, Loss 0.00023987231543287635\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "Step 2400, Loss 0.00024798428057692945\n",
            "Starting epoch 63/70, LR = [0.08000000000000002]\n",
            "Step 2450, Loss 0.0003246738633606583\n",
            "Starting epoch 64/70, LR = [0.08000000000000002]\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "Step 2500, Loss 0.0002458093222230673\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "Step 2550, Loss 0.0001327661011600867\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "Step 2600, Loss 0.0004245068703312427\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "Step 2650, Loss 0.000298970437142998\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "Step 2700, Loss 0.000164203011081554\n",
            "-----> Score: 1.0, config:{'C': 1, 'kernel': 'linear'}\n",
            "Number of SVM-exemplars: 233\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Construct Exemplar\n",
            "Computing Means\n",
            "classifing\n",
            "Test Accuracy: 0.843\n",
            "processing batch 2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Starting epoch 1/70, LR = [2]\n",
            "Step 0, Loss 0.1325831413269043\n",
            "Step 50, Loss 0.026393316686153412\n",
            "Starting epoch 2/70, LR = [2]\n",
            "Step 100, Loss 0.025516195222735405\n",
            "Starting epoch 3/70, LR = [2]\n",
            "Step 150, Loss 0.023289570584893227\n",
            "Starting epoch 4/70, LR = [2]\n",
            "Step 200, Loss 0.025326622650027275\n",
            "Starting epoch 5/70, LR = [2]\n",
            "Step 250, Loss 0.0206911563873291\n",
            "Starting epoch 6/70, LR = [2]\n",
            "Step 300, Loss 0.01825886033475399\n",
            "Starting epoch 7/70, LR = [2]\n",
            "Step 350, Loss 0.01793498918414116\n",
            "Starting epoch 8/70, LR = [2]\n",
            "Step 400, Loss 0.016659636050462723\n",
            "Starting epoch 9/70, LR = [2]\n",
            "Step 450, Loss 0.017511894926428795\n",
            "Starting epoch 10/70, LR = [2]\n",
            "Step 500, Loss 0.01890753209590912\n",
            "Starting epoch 11/70, LR = [2]\n",
            "Step 550, Loss 0.014529387466609478\n",
            "Starting epoch 12/70, LR = [2]\n",
            "Step 600, Loss 0.016231298446655273\n",
            "Starting epoch 13/70, LR = [2]\n",
            "Step 650, Loss 0.016966989263892174\n",
            "Step 700, Loss 0.01531868427991867\n",
            "Starting epoch 14/70, LR = [2]\n",
            "Step 750, Loss 0.014109241776168346\n",
            "Starting epoch 15/70, LR = [2]\n",
            "Step 800, Loss 0.015359773300588131\n",
            "Starting epoch 16/70, LR = [2]\n",
            "Step 850, Loss 0.014561488293111324\n",
            "Starting epoch 17/70, LR = [2]\n",
            "Step 900, Loss 0.014959299005568027\n",
            "Starting epoch 18/70, LR = [2]\n",
            "Step 950, Loss 0.01430569402873516\n",
            "Starting epoch 19/70, LR = [2]\n",
            "Step 1000, Loss 0.014505851082503796\n",
            "Starting epoch 20/70, LR = [2]\n",
            "Step 1050, Loss 0.015478449873626232\n",
            "Starting epoch 21/70, LR = [2]\n",
            "Step 1100, Loss 0.013750243932008743\n",
            "Starting epoch 22/70, LR = [2]\n",
            "Step 1150, Loss 0.013645105063915253\n",
            "Starting epoch 23/70, LR = [2]\n",
            "Step 1200, Loss 0.01434696838259697\n",
            "Starting epoch 24/70, LR = [2]\n",
            "Step 1250, Loss 0.014068694785237312\n",
            "Starting epoch 25/70, LR = [2]\n",
            "Step 1300, Loss 0.012940878979861736\n",
            "Starting epoch 26/70, LR = [2]\n",
            "Step 1350, Loss 0.011446570977568626\n",
            "Step 1400, Loss 0.012967409566044807\n",
            "Starting epoch 27/70, LR = [2]\n",
            "Step 1450, Loss 0.012507707811892033\n",
            "Starting epoch 28/70, LR = [2]\n",
            "Step 1500, Loss 0.012057509273290634\n",
            "Starting epoch 29/70, LR = [2]\n",
            "Step 1550, Loss 0.013739366084337234\n",
            "Starting epoch 30/70, LR = [2]\n",
            "Step 1600, Loss 0.013599898666143417\n",
            "Starting epoch 31/70, LR = [2]\n",
            "Step 1650, Loss 0.012639511376619339\n",
            "Starting epoch 32/70, LR = [2]\n",
            "Step 1700, Loss 0.012033416889607906\n",
            "Starting epoch 33/70, LR = [2]\n",
            "Step 1750, Loss 0.014472303912043571\n",
            "Starting epoch 34/70, LR = [2]\n",
            "Step 1800, Loss 0.012677359394729137\n",
            "Starting epoch 35/70, LR = [2]\n",
            "Step 1850, Loss 0.013451538980007172\n",
            "Starting epoch 36/70, LR = [2]\n",
            "Step 1900, Loss 0.013049356639385223\n",
            "Starting epoch 37/70, LR = [2]\n",
            "Step 1950, Loss 0.013158230111002922\n",
            "Starting epoch 38/70, LR = [2]\n",
            "Step 2000, Loss 0.011589976027607918\n",
            "Step 2050, Loss 0.011665323749184608\n",
            "Starting epoch 39/70, LR = [2]\n",
            "Step 2100, Loss 0.012132889591157436\n",
            "Starting epoch 40/70, LR = [2]\n",
            "Step 2150, Loss 0.013421948067843914\n",
            "Starting epoch 41/70, LR = [2]\n",
            "Step 2200, Loss 0.011959590949118137\n",
            "Starting epoch 42/70, LR = [2]\n",
            "Step 2250, Loss 0.012983041815459728\n",
            "Starting epoch 43/70, LR = [2]\n",
            "Step 2300, Loss 0.012756365351378918\n",
            "Starting epoch 44/70, LR = [2]\n",
            "Step 2350, Loss 0.01083772350102663\n",
            "Starting epoch 45/70, LR = [2]\n",
            "Step 2400, Loss 0.012215220369398594\n",
            "Starting epoch 46/70, LR = [2]\n",
            "Step 2450, Loss 0.0107180867344141\n",
            "Starting epoch 47/70, LR = [2]\n",
            "Step 2500, Loss 0.010083732195198536\n",
            "Starting epoch 48/70, LR = [2]\n",
            "Step 2550, Loss 0.012391862459480762\n",
            "Starting epoch 49/70, LR = [0.4]\n",
            "Step 2600, Loss 0.011856007389724255\n",
            "Starting epoch 50/70, LR = [0.4]\n",
            "Step 2650, Loss 0.010003719478845596\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "Step 2700, Loss 0.010047858580946922\n",
            "Step 2750, Loss 0.009761194698512554\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "Step 2800, Loss 0.0102031659334898\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "Step 2850, Loss 0.00966570619493723\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "Step 2900, Loss 0.011074232868850231\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "Step 2950, Loss 0.009649498388171196\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "Step 3000, Loss 0.009392833337187767\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "Step 3050, Loss 0.008805539458990097\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "Step 3100, Loss 0.009786241687834263\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "Step 3150, Loss 0.00942093227058649\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "Step 3200, Loss 0.010239913128316402\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "Step 3250, Loss 0.009676334448158741\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "Step 3300, Loss 0.009779361076653004\n",
            "Starting epoch 63/70, LR = [0.08000000000000002]\n",
            "Step 3350, Loss 0.009093595668673515\n",
            "Step 3400, Loss 0.009869528003036976\n",
            "Starting epoch 64/70, LR = [0.08000000000000002]\n",
            "Step 3450, Loss 0.008829559199512005\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "Step 3500, Loss 0.009196885861456394\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "Step 3550, Loss 0.008992660790681839\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "Step 3600, Loss 0.009566569700837135\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "Step 3650, Loss 0.009270084090530872\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "Step 3700, Loss 0.009645028039813042\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "Step 3750, Loss 0.009770241566002369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-04d1a63a21d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_big\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt\u001b[0m \u001b[0;31m#numbers of exemplars per class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlenx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleny\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-60e9cddbd8df>\u001b[0m in \u001b[0;36mupdate_representation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m       \u001b[0mfeature_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m       \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeature_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"list\") to tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qaeqgqc2Xr1r",
        "colab_type": "text"
      },
      "source": [
        "**Define plot function**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr8d6IXeXrJt",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_plot(accuracies):\n",
        "  ### FOR MEAN STD PLOT https://stackoverflow.com/questions/22481854/plot-mean-and-standard-deviation\n",
        "  from scipy import interpolate\n",
        "\n",
        "  tck,u     = interpolate.splprep( [[i*10 for i in range(1,len(accuracies)+1)],accuracies] ,s = 0 )\n",
        "  xnew,ynew = interpolate.splev( np.linspace( 0, 1, 100 ), tck,der = 0)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(15,14), facecolor='white')\n",
        "\n",
        "  plt.rc('font', size=20)\n",
        "  plt.plot( [i*10 for i in range(1,len(accuracies)+1)],accuracies,'.' , xnew ,ynew, label = \"accuracy\", c='orange' )\n",
        "  ax.set_ylabel(\"Accuracy\")\n",
        "  ax.set_xlabel(\"Classes\")\n",
        "  ax.minorticks_on()\n",
        "  plt.title(\"Accuracies obtained with Icarl\")\n",
        "  plt.yticks(np.arange(0, 1.1, .1))\n",
        "  plt.xticks(np.arange(0, 110, 10))\n",
        "  plt.grid(axis='y',which='major', linestyle='-', linewidth='0.5', color='black') \n",
        "  plt.grid(axis='y',which='minor', linestyle=':', linewidth='0.5', color='grey')\n",
        "  for in_i, in_j in zip([i*10 for i in range(1,len(accuracies)+1)], accuracies):  # Plot also the value of the point close to it\n",
        "          ax.annotate(str(round(in_j, 3)), xy=(in_i, in_j))\n",
        "\n",
        "  plt.savefig('test.png', format='png', dpi=300)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VphrtfF8XqsN",
        "colab_type": "text"
      },
      "source": [
        "*Print & Plot*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6Asycu5ddm8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(accuracies)\n",
        "\n",
        "accuracy_plot(accuracies)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}