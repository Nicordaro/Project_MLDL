{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_MLDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicordaro/Project_MLDL/blob/master/Project_MLDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZYQvS3r5xTI",
        "colab_type": "text"
      },
      "source": [
        "**Politecnico di Torino**\n",
        "\n",
        "**01TXFSM - Machine learning and Deep learning**\n",
        "\n",
        "**Incremental Learning in Image Classification**\n",
        "\n",
        "**Cordaro Nicol√≤ - s272145**\n",
        "\n",
        "**Di Nepi Marco - sMATRICOLA**\n",
        "\n",
        "**Falletta Alberto - s277971**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2p8HF2uz69h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install 'torch==1.4.0'\n",
        "# !pip3 install 'torchvision==0.5.0'\n",
        "# !pip3 install 'Pillow-SIMD'\n",
        "# !pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZiyg_uxCzrX",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waVhyQpu0xCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBXEJBSFC2vv",
        "colab_type": "text"
      },
      "source": [
        "**Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3w4eQ0z6smK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "# Init at 10 because first train is on 10 classes\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Used for the pseudorandom shuffle of the split\n",
        "SEED = 42\n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-3            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz6lTECKCnrn",
        "colab_type": "text"
      },
      "source": [
        "**Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca0Dsj2e6zRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))                                 \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jurLa0kcDVAd",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**\n",
        "\n",
        "CIFAR100 has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another.\n",
        "\n",
        "Each of the downloaded files is a Python \"pickled\" object produced with cPickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I23d3NGY31hd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "653559f9-8f4f-4b8c-f93b-1419271649de"
      },
      "source": [
        "# Clone github repository with data\n",
        "if os.path.isdir('./Project_MLDL'):\n",
        "  !rm -rf Project_MLDL\n",
        "if not os.path.isdir('./CIFAR100_tError'):\n",
        "  !git clone https://github.com/Nicordaro/Project_MLDL\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Project_MLDL'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/50)\u001b[K\rremote: Counting objects:   4% (2/50)\u001b[K\rremote: Counting objects:   6% (3/50)\u001b[K\rremote: Counting objects:   8% (4/50)\u001b[K\rremote: Counting objects:  10% (5/50)\u001b[K\rremote: Counting objects:  12% (6/50)\u001b[K\rremote: Counting objects:  14% (7/50)\u001b[K\rremote: Counting objects:  16% (8/50)\u001b[K\rremote: Counting objects:  18% (9/50)\u001b[K\rremote: Counting objects:  20% (10/50)\u001b[K\rremote: Counting objects:  22% (11/50)\u001b[K\rremote: Counting objects:  24% (12/50)\u001b[K\rremote: Counting objects:  26% (13/50)\u001b[K\rremote: Counting objects:  28% (14/50)\u001b[K\rremote: Counting objects:  30% (15/50)\u001b[K\rremote: Counting objects:  32% (16/50)\u001b[K\rremote: Counting objects:  34% (17/50)\u001b[K\rremote: Counting objects:  36% (18/50)\u001b[K\rremote: Counting objects:  38% (19/50)\u001b[K\rremote: Counting objects:  40% (20/50)\u001b[K\rremote: Counting objects:  42% (21/50)\u001b[K\rremote: Counting objects:  44% (22/50)\u001b[K\rremote: Counting objects:  46% (23/50)\u001b[K\rremote: Counting objects:  48% (24/50)\u001b[K\rremote: Counting objects:  50% (25/50)\u001b[K\rremote: Counting objects:  52% (26/50)\u001b[K\rremote: Counting objects:  54% (27/50)\u001b[K\rremote: Counting objects:  56% (28/50)\u001b[K\rremote: Counting objects:  58% (29/50)\u001b[K\rremote: Counting objects:  60% (30/50)\u001b[K\rremote: Counting objects:  62% (31/50)\u001b[K\rremote: Counting objects:  64% (32/50)\u001b[K\rremote: Counting objects:  66% (33/50)\u001b[K\rremote: Counting objects:  68% (34/50)\u001b[K\rremote: Counting objects:  70% (35/50)\u001b[K\rremote: Counting objects:  72% (36/50)\u001b[K\rremote: Counting objects:  74% (37/50)\u001b[K\rremote: Counting objects:  76% (38/50)\u001b[K\rremote: Counting objects:  78% (39/50)\u001b[K\rremote: Counting objects:  80% (40/50)\u001b[K\rremote: Counting objects:  82% (41/50)\u001b[K\rremote: Counting objects:  84% (42/50)\u001b[K\rremote: Counting objects:  86% (43/50)\u001b[K\rremote: Counting objects:  88% (44/50)\u001b[K\rremote: Counting objects:  90% (45/50)\u001b[K\rremote: Counting objects:  92% (46/50)\u001b[K\rremote: Counting objects:  94% (47/50)\u001b[K\rremote: Counting objects:  96% (48/50)\u001b[K\rremote: Counting objects:  98% (49/50)\u001b[K\rremote: Counting objects: 100% (50/50)\u001b[K\rremote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/48)\u001b[K\rremote: Compressing objects:   4% (2/48)\u001b[K\rremote: Compressing objects:   6% (3/48)\u001b[K\rremote: Compressing objects:   8% (4/48)\u001b[K\rremote: Compressing objects:  10% (5/48)\u001b[K\rremote: Compressing objects:  12% (6/48)\u001b[K\rremote: Compressing objects:  14% (7/48)\u001b[K\rremote: Compressing objects:  16% (8/48)\u001b[K\rremote: Compressing objects:  18% (9/48)\u001b[K\rremote: Compressing objects:  20% (10/48)\u001b[K\rremote: Compressing objects:  22% (11/48)\u001b[K\rremote: Compressing objects:  25% (12/48)\u001b[K\rremote: Compressing objects:  27% (13/48)\u001b[K\rremote: Compressing objects:  29% (14/48)\u001b[K\rremote: Compressing objects:  31% (15/48)\u001b[K\rremote: Compressing objects:  33% (16/48)\u001b[K\rremote: Compressing objects:  35% (17/48)\u001b[K\rremote: Compressing objects:  37% (18/48)\u001b[K\rremote: Compressing objects:  39% (19/48)\u001b[K\rremote: Compressing objects:  41% (20/48)\u001b[K\rremote: Compressing objects:  43% (21/48)\u001b[K\rremote: Compressing objects:  45% (22/48)\u001b[K\rremote: Compressing objects:  47% (23/48)\u001b[K\rremote: Compressing objects:  50% (24/48)\u001b[K\rremote: Compressing objects:  52% (25/48)\u001b[K\rremote: Compressing objects:  54% (26/48)\u001b[K\rremote: Compressing objects:  56% (27/48)\u001b[K\rremote: Compressing objects:  58% (28/48)\u001b[K\rremote: Compressing objects:  60% (29/48)\u001b[K\rremote: Compressing objects:  62% (30/48)\u001b[K\rremote: Compressing objects:  64% (31/48)\u001b[K\rremote: Compressing objects:  66% (32/48)\u001b[K\rremote: Compressing objects:  68% (33/48)\u001b[K\rremote: Compressing objects:  70% (34/48)\u001b[K\rremote: Compressing objects:  72% (35/48)\u001b[K\rremote: Compressing objects:  75% (36/48)\u001b[K\rremote: Compressing objects:  77% (37/48)\u001b[K\rremote: Compressing objects:  79% (38/48)\u001b[K\rremote: Compressing objects:  81% (39/48)\u001b[K\rremote: Compressing objects:  83% (40/48)\u001b[K\rremote: Compressing objects:  85% (41/48)\u001b[K\rremote: Compressing objects:  87% (42/48)\u001b[K\rremote: Compressing objects:  89% (43/48)\u001b[K\rremote: Compressing objects:  91% (44/48)\u001b[K\rremote: Compressing objects:  93% (45/48)\u001b[K\rremote: Compressing objects:  95% (46/48)\u001b[K\rremote: Compressing objects:  97% (47/48)\u001b[K\rremote: Compressing objects: 100% (48/48)\u001b[K\rremote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "Unpacking objects:   2% (1/50)   \rUnpacking objects:   4% (2/50)   \rUnpacking objects:   6% (3/50)   \rUnpacking objects:   8% (4/50)   \rUnpacking objects:  10% (5/50)   \rUnpacking objects:  12% (6/50)   \rUnpacking objects:  14% (7/50)   \rUnpacking objects:  16% (8/50)   \rUnpacking objects:  18% (9/50)   \rUnpacking objects:  20% (10/50)   \rUnpacking objects:  22% (11/50)   \rUnpacking objects:  24% (12/50)   \rUnpacking objects:  26% (13/50)   \rUnpacking objects:  28% (14/50)   \rUnpacking objects:  30% (15/50)   \rUnpacking objects:  32% (16/50)   \rUnpacking objects:  34% (17/50)   \rUnpacking objects:  36% (18/50)   \rUnpacking objects:  38% (19/50)   \rUnpacking objects:  40% (20/50)   \rUnpacking objects:  42% (21/50)   \rUnpacking objects:  44% (22/50)   \rUnpacking objects:  46% (23/50)   \rUnpacking objects:  48% (24/50)   \rUnpacking objects:  50% (25/50)   \rUnpacking objects:  52% (26/50)   \rUnpacking objects:  54% (27/50)   \rUnpacking objects:  56% (28/50)   \rUnpacking objects:  58% (29/50)   \rUnpacking objects:  60% (30/50)   \rUnpacking objects:  62% (31/50)   \rUnpacking objects:  64% (32/50)   \rUnpacking objects:  66% (33/50)   \rUnpacking objects:  68% (34/50)   \rUnpacking objects:  70% (35/50)   \rUnpacking objects:  72% (36/50)   \rUnpacking objects:  74% (37/50)   \rUnpacking objects:  76% (38/50)   \rremote: Total 50 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  78% (39/50)   \rUnpacking objects:  80% (40/50)   \rUnpacking objects:  82% (41/50)   \rUnpacking objects:  84% (42/50)   \rUnpacking objects:  86% (43/50)   \rUnpacking objects:  88% (44/50)   \rUnpacking objects:  90% (45/50)   \rUnpacking objects:  92% (46/50)   \rUnpacking objects:  94% (47/50)   \rUnpacking objects:  96% (48/50)   \rUnpacking objects:  98% (49/50)   \rUnpacking objects: 100% (50/50)   \rUnpacking objects: 100% (50/50), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvpCa6oj_ObK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Project_MLDL.CIFAR100_tError import CIFAR100_tError\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "DATA_DIR = './CIFAR100'\n",
        "\n",
        "lbls = [i for i in range(0,100)]\n",
        "random.seed(SEED)\n",
        "random.shuffle(lbls)\n",
        "\n",
        "added_labels=[]\n",
        "new_labels=[]\n",
        "\n",
        "def make_data_labels(lbls):\n",
        "  new_labels=[]\n",
        "  for el in lbls[:10]:\n",
        "    added_labels.append(el)\n",
        "    new_labels.append(el)\n",
        "  lbls = lbls[10:]\n",
        "\n",
        "  return added_labels, lbls, new_labels\n",
        "\n",
        "def increment_dataset(new_labels):\n",
        "  CIFAR100_tError.increment(new_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeaVyv7yNpzq",
        "colab_type": "text"
      },
      "source": [
        "data -- a 50000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
        "\n",
        "labels -- a list of 50000 numbers in the range 0-99. The number at index i indicates the label of the ith image in the array data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfwM-kfBIRwY",
        "colab_type": "text"
      },
      "source": [
        "**Create dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMp8TaXOIWnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "2066c241-4191-4ff2-f68c-5d5f3507bc86"
      },
      "source": [
        "if not os.path.isdir('./CIFAR100'):\n",
        "  train_dataset = CIFAR100_tError(DATA_DIR, train=True, transform=None, target_transform=None, download=True)\n",
        "  test_dataset = CIFAR100_tError(DATA_DIR, train=False, transform=None, target_transform=None, download=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-67378050154e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./CIFAR100'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR100_tError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR100_tError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'target_transform'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nqBUIREdade",
        "colab_type": "text"
      },
      "source": [
        "**Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWnV_CTodYHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq7VaNDZd16T",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VcIBkSsd2Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = resnet18(pretrained=False)\n",
        "net.fc = nn.Linear(512, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXm99r7jeCR6",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz2-zJYNeGA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Parameters to optimize:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "parameters_to_optimize = net.parameters()\n",
        "\n",
        "# Optimizers\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common scheduler is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}