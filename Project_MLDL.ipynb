{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_MLDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicordaro/Project_MLDL/blob/master/Project_MLDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZYQvS3r5xTI",
        "colab_type": "text"
      },
      "source": [
        "**Politecnico di Torino**\n",
        "\n",
        "**01TXFSM - Machine learning and Deep learning**\n",
        "\n",
        "**Incremental Learning in Image Classification**\n",
        "\n",
        "**Cordaro Nicolò - s272145**\n",
        "\n",
        "**Di Nepi Marco - sMATRICOLA**\n",
        "\n",
        "**Falletta Alberto - s277971**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2p8HF2uz69h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install 'torch==1.4.0'\n",
        "# !pip3 install 'torchvision==0.5.0'\n",
        "# !pip3 install 'Pillow-SIMD'\n",
        "# !pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZiyg_uxCzrX",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waVhyQpu0xCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBXEJBSFC2vv",
        "colab_type": "text"
      },
      "source": [
        "**Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3w4eQ0z6smK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "# Init at 10 because first train is on 10 classes\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Used for the pseudorandom shuffle of the split\n",
        "SEED = 42\n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.1            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 10      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz6lTECKCnrn",
        "colab_type": "text"
      },
      "source": [
        "**Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca0Dsj2e6zRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(224),      # Resizes short size of the PIL image to 256\n",
        "                                      #transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(224),\n",
        "                                      #transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))                                 \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jurLa0kcDVAd",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**\n",
        "\n",
        "CIFAR100 has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another.\n",
        "\n",
        "Each of the downloaded files is a Python \"pickled\" object produced with cPickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I23d3NGY31hd",
        "colab_type": "code",
        "outputId": "cad1cc2f-3b55-4be9-9607-0ba1a5394096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "# if os.path.isdir('./Project_MLDL'):\n",
        "#   !rm -rf Project_MLDL\n",
        "if not os.path.isdir('./CIFAR100_tError'):\n",
        "  !git clone https://github.com/Nicordaro/Project_MLDL\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Project_MLDL' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvpCa6oj_ObK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Project_MLDL.CIFAR100_tError import CIFAR100_tError\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "DATA_DIR = './CIFAR100'\n",
        "\n",
        "lbls = [i for i in range(0,100)]\n",
        "# random.seed(SEED)\n",
        "# random.shuffle(lbls)\n",
        "\n",
        "added_labels=[]\n",
        "\n",
        "def make_data_labels(lbls, added_labels):\n",
        "  new_labels=[]\n",
        "  for el in lbls[:10]:\n",
        "    added_labels.append(el)\n",
        "    new_labels.append(el)\n",
        "  lbls = lbls[10:]\n",
        "\n",
        "  return added_labels, lbls, new_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeaVyv7yNpzq",
        "colab_type": "text"
      },
      "source": [
        "data -- a 50000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
        "\n",
        "labels -- a list of 50000 numbers in the range 0-99. The number at index i indicates the label of the ith image in the array data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfwM-kfBIRwY",
        "colab_type": "text"
      },
      "source": [
        "**Create dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMp8TaXOIWnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f475f082-4408-47b1-d26b-1ed0945ebc4a"
      },
      "source": [
        "# if not os.path.isdir('./CIFAR100'):\n",
        "train_dataset = CIFAR100_tError(DATA_DIR, train=True, transform=train_transform, download=True)\n",
        "test_dataset = CIFAR100_tError(DATA_DIR, train=False, transform=eval_transform, download=True)\n",
        "#FOR DEBUGGING PURPOSES\n",
        "# lista = [1, 9, 15, 41, 42, 50, 65, 70, 78, 91]\n",
        "# train_dataset.increment(lista)\n",
        "# test_dataset.increment(lista)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF_sHkEfTR9O",
        "colab_type": "text"
      },
      "source": [
        "############# **AFTER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nqBUIREdade",
        "colab_type": "text"
      },
      "source": [
        "**Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWnV_CTodYHX",
        "colab_type": "code",
        "outputId": "8454f1ce-bcf4-4701-acc1-d96f12e37420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "added_labels, lbls, new_labels = make_data_labels(lbls, added_labels)\n",
        "train_dataset.increment(new_labels) #load 10 classes\n",
        "test_dataset.increment(new_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "lbl_train = []\n",
        "for el in train_dataset:\n",
        "  lbl_train.append(el[1])\n",
        "print(np.unique(lbl_train))\n",
        "lbl_test =[]\n",
        "for el in test_dataset:\n",
        "  lbl_test.append(el[1])\n",
        "print(np.unique(lbl_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq7VaNDZd16T",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VcIBkSsd2Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXm99r7jeCR6",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz2-zJYNeGA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Parameters to optimize:\n",
        "parameters_to_optimize = net.parameters()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "# Scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov78HHdLtxTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "0946f29f-b3b2-407f-8d37-7dcf0bad35c6"
      },
      "source": [
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "  \n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/10, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 0, Loss 2.439375877380371\n",
            "Step 10, Loss 1.186208963394165\n",
            "Starting epoch 2/10, LR = [0.1]\n",
            "Step 20, Loss 1.4998663663864136\n",
            "Step 30, Loss 1.4990170001983643\n",
            "Starting epoch 3/10, LR = [0.1]\n",
            "Step 40, Loss 1.0352461338043213\n",
            "Step 50, Loss 0.7446833252906799\n",
            "Starting epoch 4/10, LR = [0.1]\n",
            "Step 60, Loss 0.36517950892448425\n",
            "Step 70, Loss 0.5476040840148926\n",
            "Starting epoch 5/10, LR = [0.1]\n",
            "Step 80, Loss 0.25593721866607666\n",
            "Step 90, Loss 0.1837647706270218\n",
            "Starting epoch 6/10, LR = [0.1]\n",
            "Step 100, Loss 0.1935044825077057\n",
            "Step 110, Loss 0.13393479585647583\n",
            "Starting epoch 7/10, LR = [0.1]\n",
            "Step 120, Loss 0.05802091956138611\n",
            "Step 130, Loss 0.17436042428016663\n",
            "Starting epoch 8/10, LR = [0.1]\n",
            "Step 140, Loss 0.10789696127176285\n",
            "Step 150, Loss 0.13515301048755646\n",
            "Starting epoch 9/10, LR = [0.1]\n",
            "Step 160, Loss 0.058009691536426544\n",
            "Step 170, Loss 0.025526225566864014\n",
            "Starting epoch 10/10, LR = [0.1]\n",
            "Step 180, Loss 0.0506381057202816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asqyGOuPtzcz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxHtmZ2ft0Fi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b678ae46-2f8f-4671-a6a4-23fb596c1dc1"
      },
      "source": [
        "#test phase\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:02<00:00,  1.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}