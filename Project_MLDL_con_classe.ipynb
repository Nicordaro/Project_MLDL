{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_MLDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicordaro/Project_MLDL/blob/master/Project_MLDL_con_classe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZYQvS3r5xTI",
        "colab_type": "text"
      },
      "source": [
        "**Politecnico di Torino**\n",
        "\n",
        "**01TXFSM - Machine learning and Deep learning**\n",
        "\n",
        "**Incremental Learning in Image Classification**\n",
        "\n",
        "**Cordaro Nicol√≤ - s272145**\n",
        "\n",
        "**Di Nepi Marco - s277959**\n",
        "\n",
        "**Falletta Alberto - s277971**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2p8HF2uz69h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install 'torch==1.4.0'\n",
        "# !pip3 install 'torchvision==0.5.0'\n",
        "# !pip3 install 'Pillow-SIMD'\n",
        "# !pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZiyg_uxCzrX",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waVhyQpu0xCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBXEJBSFC2vv",
        "colab_type": "text"
      },
      "source": [
        "**Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3w4eQ0z6smK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "SEED = 42 # Used for the pseudorandom shuffle of the split\n",
        "\n",
        "BATCH_SIZE = 256  \n",
        "LR = 0.01           \n",
        "MOMENTUM = 0.9       \n",
        "WEIGHT_DECAY = 5e-5  \n",
        "\n",
        "NUM_EPOCHS = 15  \n",
        "STEP_SIZE = 20       \n",
        "GAMMA = 0.1\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz6lTECKCnrn",
        "colab_type": "text"
      },
      "source": [
        "**Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca0Dsj2e6zRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      \n",
        "                                      transforms.CenterCrop(224),         \n",
        "                                      transforms.ToTensor(), \n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
        "                                      ])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                 \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jurLa0kcDVAd",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**\n",
        "\n",
        "CIFAR100 has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another.\n",
        "\n",
        "Each of the downloaded files is a Python \"pickled\" object produced with cPickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I23d3NGY31hd",
        "colab_type": "code",
        "outputId": "a31661e7-00c8-4256-b9af-fe19de37d0ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "# if os.path.isdir('./Project_MLDL'):\n",
        "#   !rm -rf Project_MLDL\n",
        "if not os.path.isdir('./CIFAR100_tError'):\n",
        "  !git clone https://github.com/Nicordaro/Project_MLDL\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Project_MLDL' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeaVyv7yNpzq",
        "colab_type": "text"
      },
      "source": [
        "data -- a 50000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
        "\n",
        "labels -- a list of 50000 numbers in the range 0-99. The number at index i indicates the label of the ith image in the array data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfwM-kfBIRwY",
        "colab_type": "text"
      },
      "source": [
        "**Create dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi0NmczTdxQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import PIL.Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "\n",
        "# from .utils import check_integrity, download_and_extract_archive\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "def pil_loader(f):\n",
        "    # open file\n",
        "    img = PIL.Image.open(f)\n",
        "    return img.convert('RGB')\n",
        "\n",
        "\n",
        "class CIFAR100_tError(CIFAR100):\n",
        "\n",
        "  def __init__(self, root, train=True, transform=None,download=False, lbls=[]):\n",
        "    flag = train\n",
        "    self.prova = CIFAR100(root, train=train, download=download)\n",
        "    self.transform = transform\n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "    for element in self.prova:\n",
        "      image, label = element\n",
        "      if label in lbls:\n",
        "        self.data.append(image)\n",
        "        self.labels.append(label)\n",
        "\n",
        "  def __len__(self):\n",
        "        '''\n",
        "        The __len__ method returns the length of the dataset\n",
        "        It is mandatory, as this is used by several other components\n",
        "        '''\n",
        "        length = len(self.data)\n",
        "        return length\n",
        "      \n",
        "          \n",
        "  def __getitem__(self, index):\n",
        "      '''\n",
        "      __getitem__ should access an element through its index\n",
        "      Args:\n",
        "          index (int): Index\n",
        "      Returns:\n",
        "          tuple: (sample, target) where target is class_index of the target class.\n",
        "      '''\n",
        "      \n",
        "      image = self.data[index]\n",
        "      label = self.labels[index]\n",
        "      \n",
        "      if self.transform is not None:\n",
        "          image = self.transform(image)\n",
        "      \n",
        "      return image, label\n",
        "    \n",
        "  def increment(self, newlbls):\n",
        "    for element in self.prova:\n",
        "      image, label = element\n",
        "      if label in newlbls:\n",
        "        self.data.append(image)\n",
        "        self.labels.append(label)\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo44T0uFeaLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from Project_MLDL.CIFAR100_tError import CIFAR100_tError\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "DATA_DIR = './CIFAR100'\n",
        "\n",
        "lbls = [i for i in range(0,100)]\n",
        "random.seed(SEED)\n",
        "random.shuffle(lbls)\n",
        "\n",
        "added_labels=[]\n",
        "new_labels=[]\n",
        "\n",
        "def make_data_labels(lbls):\n",
        "  new_labels=[]\n",
        "  for el in lbls[:10]:\n",
        "    added_labels.append(el)\n",
        "    new_labels.append(el)\n",
        "  lbls = lbls[10:]\n",
        "\n",
        "  return added_labels, lbls, new_labels\n",
        "\n",
        "def increment_dataset(new_labels):\n",
        "  CIFAR100_tError.increment(new_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMp8TaXOIWnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4c03cccb-c86d-47d3-fa9a-3f0311034dac"
      },
      "source": [
        "train_dataset = CIFAR100_tError(DATA_DIR, train=True, transform=train_transform, download=True)\n",
        "test_dataset = CIFAR100_tError(DATA_DIR, train=False, transform=eval_transform, download=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nqBUIREdade",
        "colab_type": "text"
      },
      "source": [
        "**Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWnV_CTodYHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "added_labels, lbls, new_labels = make_data_labels(lbls)\n",
        "train_dataset.increment(new_labels) #load 10 classes\n",
        "test_dataset.increment(new_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq7VaNDZd16T",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VcIBkSsd2Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXm99r7jeCR6",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz2-zJYNeGA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Parameters to optimize:\n",
        "parameters_to_optimize = net.parameters()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "# Scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2JbZwFRWJWy",
        "colab_type": "text"
      },
      "source": [
        "**Training**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwoYvqozWIl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "outputId": "840613d3-b5d5-48cd-8fb4-6c8c0759e0e3"
      },
      "source": [
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "  \n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/15, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 0, Loss 2.527050733566284\n",
            "Step 10, Loss 1.96030592918396\n",
            "Starting epoch 2/15, LR = [0.01]\n",
            "Step 20, Loss 1.9993001222610474\n",
            "Step 30, Loss 1.962334156036377\n",
            "Starting epoch 3/15, LR = [0.01]\n",
            "Step 40, Loss 1.8523123264312744\n",
            "Step 50, Loss 1.9852763414382935\n",
            "Starting epoch 4/15, LR = [0.01]\n",
            "Step 60, Loss 1.985669493675232\n",
            "Step 70, Loss 1.8978134393692017\n",
            "Starting epoch 5/15, LR = [0.01]\n",
            "Step 80, Loss 1.8702114820480347\n",
            "Step 90, Loss 1.5893949270248413\n",
            "Starting epoch 6/15, LR = [0.01]\n",
            "Step 100, Loss 1.8949542045593262\n",
            "Step 110, Loss 1.8960829973220825\n",
            "Starting epoch 7/15, LR = [0.01]\n",
            "Step 120, Loss 1.9786536693572998\n",
            "Step 130, Loss 1.8909844160079956\n",
            "Starting epoch 8/15, LR = [0.01]\n",
            "Step 140, Loss 1.8786568641662598\n",
            "Step 150, Loss 1.928051471710205\n",
            "Starting epoch 9/15, LR = [0.01]\n",
            "Step 160, Loss 1.7295058965682983\n",
            "Step 170, Loss 1.8996551036834717\n",
            "Starting epoch 10/15, LR = [0.01]\n",
            "Step 180, Loss 1.9703319072723389\n",
            "Starting epoch 11/15, LR = [0.01]\n",
            "Step 190, Loss 1.908834457397461\n",
            "Step 200, Loss 1.8848330974578857\n",
            "Starting epoch 12/15, LR = [0.01]\n",
            "Step 210, Loss 1.9278604984283447\n",
            "Step 220, Loss 1.9812921285629272\n",
            "Starting epoch 13/15, LR = [0.01]\n",
            "Step 230, Loss 1.8077336549758911\n",
            "Step 240, Loss 1.9007487297058105\n",
            "Starting epoch 14/15, LR = [0.01]\n",
            "Step 250, Loss 1.9162925481796265\n",
            "Step 260, Loss 1.8907430171966553\n",
            "Starting epoch 15/15, LR = [0.01]\n",
            "Step 270, Loss 1.8654159307479858\n",
            "Step 280, Loss 1.879522681236267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI1a2h71gFg_",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCXTVvZZgHb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "26e48ac4-5718-493e-b994-95ac9720f3be"
      },
      "source": [
        "#test phase\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:09<00:00,  2.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swzI1bAukqKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,10):\n",
        "  #create new train dataset\n",
        "  #increment new labels\n",
        "  #create new dataloader\n",
        "  #increment test dataset\n",
        "  #change last layer resnet\n",
        "  #train\n",
        "  #test"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}